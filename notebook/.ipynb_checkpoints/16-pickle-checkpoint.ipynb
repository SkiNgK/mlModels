{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classificar em 2 a 8 hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.fftpack import rfft, irfft, fftfreq\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = \"/home/jonnatas/Downloads/DB_REPO/edf/\"\n",
    "def dadosPK(tipo):\n",
    "    dir_ct = HOME+tipo+\"/\"\n",
    "    return os.listdir(HOME+tipo+\"/\")\n",
    "def carregarDataFrame(tipo, caminho):\n",
    "    caminho = HOME+tipo+\"/\"+caminho\n",
    "    \n",
    "    edf = pyedflib.EdfReader(caminho)\n",
    "    n = edf.signals_in_file\n",
    "    sigbufs = np.zeros((n, edf.getNSamples()[0]))\n",
    "    for i in np.arange(n):\n",
    "         sigbufs[i, :] = edf.readSignal(i)\n",
    "    edf._close()\n",
    "    del edf\n",
    "    data = sigbufs.T\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=['ch1', 'ch2', 'ch3', 'ch4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Matriz de confusão, sem normalização', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Esperado')\n",
    "    plt.xlabel('Obtido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft(df, canal):\n",
    "    amostras_validas = df[canal][2000:11000]\n",
    "    amostras = int(amostras_validas.shape[0]*500/2000)\n",
    "    sinal = np.abs(np.fft.fft(amostras_validas))[:amostras]\n",
    "    freq = np.linspace(0,500,amostras)\n",
    "\n",
    "    return (sinal,freq)\n",
    "\n",
    "def frequecia(df, canal):\n",
    "    sinalFFT = [fft(data, canal)[0] for data in df]\n",
    "    return sinalFFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaCLF(clf, trainData, rotulos, random_state=30, test_size=0.3,n_components=30, kfold=30):\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(trainData, rotulos, test_size=test_size, random_state=random_state)\n",
    "    #Raw score\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('Raw score', clf.score(X_test, y_test))\n",
    "    \n",
    "    #cross-validation\n",
    "    scores = cross_val_score(clf, trainData, rotulos, cv=kfold)      \n",
    "    print(\"score cross validation: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    #PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(trainData)\n",
    "    X_t_train = pca.transform(X_train)\n",
    "    X_t_test = pca.transform(X_test)\n",
    "    \n",
    "    clf.fit(X_t_train, y_train)\n",
    "    print('score PCA ', clf.score(X_t_test, y_test))\n",
    "   \n",
    "    #PCA + cross-validation\n",
    "    pcaCV = PCA(n_components=n_components)\n",
    "    pcaCV.fit(trainData)\n",
    "    t_trainData = pcaCV.transform(trainData)\n",
    "    #cross-validation\n",
    "    scores = cross_val_score(clf, t_trainData, rotulos, cv=kfold)   \n",
    "    print(\"score PCA cross validation: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_parkinson = dadosPK('parkinson')\n",
    "nomes_controle = dadosPK('controle')\n",
    "df_ct = [ carregarDataFrame('controle', nome) for nome in nomes_controle]\n",
    "df_pk = [ carregarDataFrame('parkinson', nome) for nome in nomes_parkinson]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrando os dados com a fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinalCT = frequecia(df_ct, 'ch1')\n",
    "sinalPK = frequecia(df_pk, 'ch1')\n",
    "\n",
    "rotulosCT = [0 for _ in sinalCT]\n",
    "rotulosPK = [1 for _ in sinalPK]\n",
    "\n",
    "trainData = sinalCT + sinalPK\n",
    "rotulos = rotulosCT + rotulosPK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando os dados (Treino, teste e validação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=0\n",
    "n_components = min(len(trainData), len(trainData[0]))\n",
    "kfold = 30\n",
    "nomes = ['controle','parkinson']\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainData, rotulos, test_size=0.3, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw score 0.7692307692307693\n",
      "score cross validation: 0.81 (+/- 0.54)\n",
      "score PCA  0.8076923076923077\n",
      "score PCA cross validation: 0.82 (+/- 0.47)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=random_state, n_estimators=50, max_features='log2', max_depth=None)\n",
    "pcaCLF(rfc, trainData, rotulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw score 0.6923076923076923\n",
      "score cross validation: 0.74 (+/- 0.56)\n",
      "score PCA  0.6538461538461539\n",
      "score PCA cross validation: 0.78 (+/- 0.62)\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=1, gamma=0.1, kernel='poly', degree=3)  \n",
    "pcaCLF(svm, trainData, rotulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the model to disk\n",
    "pickle.dump(rfc, open('rfc.sav', 'wb'))\n",
    "pickle.dump(svm, open('svm.sav', 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joblib.dump(rfc, 'rfc.sav')\n",
    "joblib.dump(svm, 'svm.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model from disk\n",
    "filename = 'rfc.sav'\n",
    "loaded_model = joblib.load(filename)\n",
    "pcaCLF(loaded_model, trainData, rotulos)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
